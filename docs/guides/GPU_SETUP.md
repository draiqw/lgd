# üöÄ GPU Version Setup Guide

–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∑–∞–ø—É—Å–∫—É GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ PABBO pipeline.

---

## –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Ç–ª–∏—á–∏—è –æ—Ç CPU –≤–µ—Ä—Å–∏–∏

### `for_klaster_gpu.py` vs `for_klaster.py`

| –ü–∞—Ä–∞–º–µ—Ç—Ä | CPU (for_klaster.py) | GPU (for_klaster_gpu.py) |
|----------|---------------------|--------------------------|
| **Small Model Training** |
| n_steps | 2000 | 4000 |
| batch_size | 16 | 32 |
| device | cpu | cuda |
| **Large Model Training** |
| n_steps | 8000 | 12000 |
| batch_size | 32 | 64 |
| device | cpu | cuda |
| **Evaluation** |
| eval_num_query_points | 256 | 512 |
| max_num_ctx | 20 | 50 |

### –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ GPU
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –∫—ç—à–∞ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏
‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è GPU
‚úÖ –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU

---

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **GPU**: NVIDIA GPU —Å 8+ GB –ø–∞–º—è—Ç–∏ (–º–∏–Ω–∏–º—É–º)
- **CUDA**: 11.0 –∏–ª–∏ –≤—ã—à–µ
- **PyTorch**: 2.0+ —Å CUDA support
- **–î—Ä–∞–π–≤–µ—Ä—ã**: NVIDIA Driver 470+

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **GPU**: NVIDIA GPU —Å 16+ GB –ø–∞–º—è—Ç–∏ (A100, V100, RTX 3090, RTX 4090)
- **CUDA**: 11.8 –∏–ª–∏ 12.1
- **PyTorch**: 2.1+ —Å CUDA support

---

## –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU

### –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ GPU
python -c "import torch; print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–∏ CUDA
python -c "import torch; print('CUDA version:', torch.version.cuda)"
```

### –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

```python
import torch

if torch.cuda.is_available():
    print("‚úì CUDA is available")
    print(f"  Device: {torch.cuda.get_device_name(0)}")
    print(f"  Device count: {torch.cuda.device_count()}")
    print(f"  CUDA version: {torch.version.cuda}")
    print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
    print("‚úó CUDA is not available")
```

---

## –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)

```bash
# –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
cd /path/to/Llabs

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ GPU –≤–µ—Ä—Å–∏—é
python for_klaster_gpu.py
```

–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
1. –ü—Ä–æ–≤–µ—Ä–∏—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
2. –í—ã–≤–µ–¥–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
3. –û–±—É—á–∏—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ GPU
4. –ó–∞–ø—É—Å—Ç–∏—Ç LDA —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
5. –°–æ–∑–¥–∞—Å—Ç –æ—Ç—á–µ—Ç—ã —Å GPU –º–µ—Ç—Ä–∏–∫–∞–º–∏

---

## –ó–∞–ø—É—Å–∫ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ

### 1. –°–æ–∑–¥–∞–π—Ç–µ SLURM —Å–∫—Ä–∏–ø—Ç –¥–ª—è GPU

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `run_gpu.sh`:

```bash
#!/bin/bash
#SBATCH --job-name=lda_gpu
#SBATCH --output=slurm-%j.out
#SBATCH --error=slurm-%j.err
#SBATCH --nodes=1
#SBATCH --cpus-per-task=32
#SBATCH --gpus=1                    # –ó–∞–ø—Ä–æ—Å–∏—Ç—å 1 GPU
#SBATCH --gres=gpu:1                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
#SBATCH --mem=64G                   # –ü–∞–º—è—Ç—å
#SBATCH --time=12:00:00             # –ú–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ —á–µ–º CPU –≤–µ—Ä—Å–∏—è

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ç–∏–ø GPU
# #SBATCH --gres=gpu:a100:1
# #SBATCH --gres=gpu:v100:1
# #SBATCH --gres=gpu:rtx3090:1

# Load modules (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –Ω–∞ –≤–∞—à–µ–º –∫–ª–∞—Å—Ç–µ—Ä–µ)
# module load cuda/11.8
# module load python/3.9

# Activate environment
source /path/to/venv/bin/activate

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
echo "==================================================================="
echo "Job started at: $(date)"
echo "Running on host: $(hostname)"
echo "CUDA visible devices: $CUDA_VISIBLE_DEVICES"
echo "==================================================================="

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
nvidia-smi

# –ó–∞–ø—É—Å–∫
python for_klaster_gpu.py

echo "==================================================================="
echo "Job finished at: $(date)"
echo "==================================================================="
```

### 2. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –∑–∞–¥–∞—á—É

```bash
sbatch run_gpu.sh
```

### 3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```bash
# –°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
squeue -u $USER

# –õ–æ–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
tail -f slurm-<jobid>.out

# GPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
ssh <compute-node> "nvidia-smi"
```

---

## Docker —Å GPU

### Dockerfile –¥–ª—è GPU

–û–±–Ω–æ–≤–∏—Ç–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π `Dockerfile`:

```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# Install Python
RUN apt-get update && apt-get install -y \
    python3.9 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements
COPY requirements.txt .

# Install PyTorch with CUDA
RUN pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install other requirements
RUN pip3 install -r requirements.txt

# Copy project files
COPY . /app

# Set entrypoint
CMD ["python3", "for_klaster_gpu.py"]
```

### –ó–∞–ø—É—Å–∫ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ —Å GPU

```bash
# –°–±–æ—Ä–∫–∞
docker build -t llabs_gpu:v1 .

# –ó–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ
docker run --gpus all -v $(pwd)/results:/app/lda_pipeline_results llabs_gpu:v1

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
docker run --gpus all llabs_gpu:v1 nvidia-smi
```

### –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Enroot (–¥–ª—è Slurm)

```bash
# –ù–∞ –º–∞—à–∏–Ω–µ —Å enroot
sudo enroot import docker://yourusername/llabs_gpu:v1
sudo enroot create --name llabs_gpu llabs_gpu.sqsh

# –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ enroot
enroot start --rw llabs_gpu python3 /app/for_klaster_gpu.py
```

---

## –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

### –ù–∞ RTX 3090 (24GB)

- **Small model training**: ~5-8 –º–∏–Ω—É—Ç (vs 30 –º–∏–Ω—É—Ç –Ω–∞ CPU)
- **Large model training**: ~15-20 –º–∏–Ω—É—Ç (vs 60 –º–∏–Ω—É—Ç –Ω–∞ CPU)
- **Total pipeline**: ~8-10 —á–∞—Å–æ–≤ (vs 20-24 —á–∞—Å–æ–≤ –Ω–∞ CPU)

### –ù–∞ A100 (40GB)

- **Small model training**: ~3-5 –º–∏–Ω—É—Ç
- **Large model training**: ~8-12 –º–∏–Ω—É—Ç
- **Total pipeline**: ~6-8 —á–∞—Å–æ–≤

### –ù–∞ V100 (32GB)

- **Small model training**: ~6-10 –º–∏–Ω—É—Ç
- **Large model training**: ~18-25 –º–∏–Ω—É—Ç
- **Total pipeline**: ~9-12 —á–∞—Å–æ–≤

---

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU

### –í–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ª–æ–≥–∏—Ä—É–µ—Ç:
- GPU device name
- CUDA version
- Total GPU memory
- Allocated memory –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞
- Reserved memory
- Max allocated memory

### –†—É—á–Ω–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

```bash
# Watch GPU usage
watch -n 1 nvidia-smi

# –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
nvidia-smi --query-gpu=timestamp,name,temperature.gpu,utilization.gpu,memory.used,memory.total --format=csv -l 1

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
nvidia-smi --query-gpu=timestamp,name,temperature.gpu,utilization.gpu,memory.used,memory.total --format=csv -l 5 > gpu_log.csv
```

---

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ï—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏

1. **–£–º–µ–Ω—å—à–∏—Ç–µ batch_size**:
   ```python
   # –í for_klaster_gpu.py, —Å—Ç—Ä–æ–∫–∏ 288-289 –∏ 295-296
   batch_size = 16  # –≤–º–µ—Å—Ç–æ 32 –¥–ª—è small
   batch_size = 32  # –≤–º–µ—Å—Ç–æ 64 –¥–ª—è large
   ```

2. **–£–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏** (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):
   ```python
   # –£–º–µ–Ω—å—à–∏—Ç–µ max_num_ctx
   data.max_num_ctx=30  # –≤–º–µ—Å—Ç–æ 50
   ```

3. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ gradient accumulation** (—Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è train.py):
   ```python
   train.gradient_accumulation_steps=2
   ```

### –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ —Å–∫–æ—Ä–æ—Å—Ç–∏

1. **–£–≤–µ–ª–∏—á—å—Ç–µ batch_size** (–µ—Å–ª–∏ –µ—Å—Ç—å –ø–∞–º—è—Ç—å):
   ```python
   batch_size = 64  # –¥–ª—è small
   batch_size = 128  # –¥–ª—è large
   ```

2. **–í–∫–ª—é—á–∏—Ç–µ mixed precision** (—Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è train.py):
   ```python
   train.use_amp=true  # Automatic Mixed Precision
   ```

3. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU** (—Ç—Ä–µ–±—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞):
   ```python
   # –î–æ–±–∞–≤—å—Ç–µ –≤ train command
   experiment.device="cuda:0,cuda:1"
   ```

---

## Troubleshooting

### CUDA Out of Memory

```
RuntimeError: CUDA out of memory
```

**–†–µ—à–µ–Ω–∏–µ**:
1. –£–º–µ–Ω—å—à–∏—Ç–µ `batch_size` (—Å–º. –≤—ã—à–µ)
2. –û—á–∏—Å—Ç–∏—Ç–µ GPU –∫—ç—à: —Å–∫—Ä–∏–ø—Ç –¥–µ–ª–∞–µ—Ç —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –µ—â–µ:
   ```python
   torch.cuda.empty_cache()
   ```
3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU —Å –±–æ–ª—å—à–µ–π –ø–∞–º—è—Ç—å—é

### GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω

```
GPU not available!
```

**–†–µ—à–µ–Ω–∏–µ**:
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CUDA installation:
   ```bash
   nvidia-smi
   nvcc --version
   ```

2. –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å CUDA:
   ```bash
   pip uninstall torch
   pip install torch --index-url https://download.pytorch.org/whl/cu118
   ```

3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CUDA_VISIBLE_DEVICES:
   ```bash
   echo $CUDA_VISIBLE_DEVICES
   ```

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –Ω–∞ GPU

**–ü—Ä–∏—á–∏–Ω—ã**:
1. –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π batch_size
2. CPU bottleneck (data loading)
3. –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è CUDA/PyTorch

**–†–µ—à–µ–Ω–∏–µ**:
1. –£–≤–µ–ª–∏—á—å—Ç–µ batch_size
2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–æ–ª–µ–µ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é PyTorch
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ GPU:
   ```python
   print(next(model.parameters()).device)  # –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å cuda:0
   ```

---

## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ `lda_pipeline_results/run_gpu_YYYYMMDD_HHMMSS/` –±—É–¥–µ—Ç:

```
logs/
‚îú‚îÄ‚îÄ pipeline_main.log           # –õ–æ–≥–∏ —Å GPU –º–µ—Ç—Ä–∏–∫–∞–º–∏
‚îî‚îÄ‚îÄ pipeline_metrics.json       # –ú–µ—Ç—Ä–∏–∫–∏ (–≤–∫–ª—é—á–∞—è GPU memory)

experiments/                     # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
aggregated_results/              # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
all_results.json                 # –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
```

–í –ª–æ–≥–∞—Ö –±—É–¥–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–∏–¥–∞:
```
‚úì GPU Available: NVIDIA A100-SXM4-40GB
‚úì GPU Count: 1
‚úì CUDA Version: 11.8
‚úì PyTorch Version: 2.1.0
‚úì Total GPU Memory: 40.00 GB
...
Before training: GPU Memory - Allocated: 0.00GB, Reserved: 0.00GB, Max: 0.00GB
After training: GPU Memory - Allocated: 2.34GB, Reserved: 3.50GB, Max: 3.45GB
```

---

## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ CPU vs GPU

| –ú–µ—Ç—Ä–∏–∫–∞ | CPU (for_klaster.py) | GPU (for_klaster_gpu.py) |
|---------|---------------------|---------------------------|
| Small training | ~30 min | ~5-8 min |
| Large training | ~60 min | ~15-20 min |
| Total pipeline | ~20-24h | ~8-10h |
| Batch size (small) | 16 | 32 |
| Batch size (large) | 32 | 64 |
| Training steps (small) | 2000 | 4000 |
| Training steps (large) | 8000 | 12000 |

**–£—Å–∫–æ—Ä–µ–Ω–∏–µ**: ~2-3x –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞

---

## –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

1. ‚úÖ **–í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ GPU** –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –¥–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
2. ‚úÖ **–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ GPU –ø–∞–º—è—Ç—å** –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
3. ‚úÖ **–û—á–∏—â–∞–π—Ç–µ GPU –∫—ç—à** –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏ (—Å–∫—Ä–∏–ø—Ç –¥–µ–ª–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
4. ‚úÖ **–õ–æ–≥–∏—Ä—É–π—Ç–µ GPU –º–µ—Ç—Ä–∏–∫–∏** –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
5. ‚úÖ **–¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω–æ** –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ
6. ‚úÖ **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π batch_size** –¥–ª—è –≤–∞—à–µ–π GPU
7. ‚úÖ **–°–ª–µ–¥–∏—Ç–µ –∑–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π GPU** (nvidia-smi)

---

## FAQ

**Q: –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU –≤–µ—Ä—Å–∏—é, –µ—Å–ª–∏ –µ—Å—Ç—å GPU?**
A: –î–∞, –Ω–æ —ç—Ç–æ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–µ–µ. GPU –≤–µ—Ä—Å–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è GPU –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è.

**Q: –°–∫–æ–ª—å–∫–æ GPU –ø–∞–º—è—Ç–∏ –Ω—É–∂–Ω–æ?**
A: –ú–∏–Ω–∏–º—É–º 8GB, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16GB –∏–ª–∏ –±–æ–ª—å—à–µ.

**Q: –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU?**
A: –¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 1 GPU. –î–ª—è multi-GPU –Ω—É–∂–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ train.py.

**Q: –†–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –Ω–∞ AMD GPU?**
A: –ù–µ—Ç, —Ç—Ä–µ–±—É–µ—Ç—Å—è NVIDIA GPU —Å CUDA.

**Q: –ß—Ç–æ –µ—Å–ª–∏ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω?**
A: –°–∫—Ä–∏–ø—Ç –≤—ã–¥–∞—Å—Ç –æ—à–∏–±–∫—É –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU –≤–µ—Ä—Å–∏—é (for_klaster.py).

---

**–ì–æ—Ç–æ–≤–æ! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU –≤–µ—Ä—Å–∏—é –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è! üöÄ**