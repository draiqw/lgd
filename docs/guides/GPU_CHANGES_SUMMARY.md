# üìã –°–≤–æ–¥–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π GPU –≤–µ—Ä—Å–∏–∏

–ö—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä –≤—Å–µ—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏.

---

## üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

| –§–∞–π–ª | –û–ø–∏—Å–∞–Ω–∏–µ |
|------|----------|
| `for_klaster_gpu.py` | GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç |
| `run_gpu.sh` | SLURM —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ GPU –Ω–æ–¥–µ |
| `GPU_SETUP.md` | –ü–æ–¥—Ä–æ–±–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é |
| `GPU_CHANGES_SUMMARY.md` | –≠—Ç–æ—Ç —Ñ–∞–π–ª - –∫—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ |

---

## üîÑ –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–¥–µ

### 1. –ö–ª–∞—Å—Å `PABBOTrainerGPU` (–≤–º–µ—Å—Ç–æ `PABBOTrainer`)

#### –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã device
```python
def __init__(self, logger, device="cuda"):  # –ë—ã–ª: –±–µ–∑ device –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
    self.device = device
```

#### –£–≤–µ–ª–∏—á–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è GPU
```python
# SMALL model
n_steps = 4000      # –ë—ã–ª–æ: 2000 –≤ CPU –≤–µ—Ä—Å–∏–∏
batch_size = 32     # –ë—ã–ª–æ: 16 –≤ CPU –≤–µ—Ä—Å–∏–∏

# LARGE model
n_steps = 12000     # –ë—ã–ª–æ: 8000 –≤ CPU –≤–µ—Ä—Å–∏–∏
batch_size = 64     # –ë—ã–ª–æ: 32 –≤ CPU –≤–µ—Ä—Å–∏–∏
```

#### –ü–µ—Ä–µ–¥–∞—á–∞ device='cuda' –≤–æ –≤—Å–µ –∫–æ–º–∞–Ω–¥—ã
```python
cmd.extend([
    f"experiment.device={self.device}",  # –ù–û–í–û–ï
    f"train.n_steps={n_steps}",          # –ù–û–í–û–ï
    f"train.train_batch_size={batch_size}",  # –ù–û–í–û–ï
])
```

#### –£–≤–µ–ª–∏—á–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö
```python
"data.max_num_ctx=50",  # –ë—ã–ª–æ: 20 –≤ CPU –≤–µ—Ä—Å–∏–∏
"eval.eval_num_query_points=512",  # –ë—ã–ª–æ: 256 –≤ CPU –≤–µ—Ä—Å–∏–∏
```

### 2. –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è GPU

#### –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
```python
def check_gpu_availability() -> Tuple[bool, Optional[str], Dict]:
    """Check if GPU is available and get GPU information."""
    import torch
    if not torch.cuda.is_available():
        return False, None, {}
    # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (available, device_name, gpu_info)
```

#### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏
```python
def get_gpu_memory_info() -> Dict:
    """Get current GPU memory usage."""
    return {
        'allocated_gb': torch.cuda.memory_allocated(0) / 1e9,
        'reserved_gb': torch.cuda.memory_reserved(0) / 1e9,
        'max_allocated_gb': torch.cuda.max_memory_allocated(0) / 1e9,
    }
```

#### –û—á–∏—Å—Ç–∫–∞ GPU –∫—ç—à–∞
```python
def clear_gpu_cache():
    """Clear GPU cache to free up memory."""
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
```

### 3. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

#### –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ `ThreadSafePipelineLogger`
```python
def log_gpu_memory(self, prefix: str = ""):
    """Log current GPU memory usage."""
    mem_info = get_gpu_memory_info()
    self.logger.info(f"{prefix}GPU Memory - Allocated: {mem_info['allocated_gb']:.2f}GB, ...")
```

#### GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –ª–æ–≥–∞—Ö —ç—Ç–∞–ø–æ–≤
```python
self.logger.log_stage(
    stage_name,
    "SUCCESS",
    training_time,
    device=self.device,  # –ù–û–í–û–ï
    gpu_memory_gb=mem_info.get('max_allocated_gb', 0)  # –ù–û–í–û–ï
)
```

### 4. –ù–æ–≤—ã–π STAGE 0 –≤ main()

```python
# =====================================================================
# STAGE 0: Check GPU Availability
# =====================================================================
gpu_available, device_name, gpu_info = check_gpu_availability()

if not gpu_available:
    raise RuntimeError("GPU not available")

pipeline_logger.log_info(f"‚úì GPU Available: {device_name}")
pipeline_logger.log_info(f"‚úì Total GPU Memory: {gpu_info['total_memory_gb']:.2f} GB")
```

### 5. –û—á–∏—Å—Ç–∫–∞ GPU –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏

```python
# –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ train_model() –∏ evaluate_model()
clear_gpu_cache()
pipeline_logger.log_info("‚úì GPU cache cleared")
```

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

### Training Parameters

| –ü–∞—Ä–∞–º–µ—Ç—Ä | CPU Version | GPU Version | –ò–∑–º–µ–Ω–µ–Ω–∏–µ |
|----------|-------------|-------------|-----------|
| **Small Model** |
| n_steps | 2000 | 4000 | +100% |
| batch_size | 16 | 32 | +100% |
| max_num_ctx | 20 | 50 | +150% |
| device | cpu | cuda | ‚úì |
| **Large Model** |
| n_steps | 8000 | 12000 | +50% |
| batch_size | 32 | 64 | +100% |
| max_num_ctx | 20 | 50 | +150% |
| device | cpu | cuda | ‚úì |
| **Evaluation** |
| eval_num_query_points | 256 | 512 | +100% |
| max_num_ctx | 20 | 50 | +150% |
| device | cpu | cuda | ‚úì |

### SLURM Parameters

| –ü–∞—Ä–∞–º–µ—Ç—Ä | run.sh (CPU) | run_gpu.sh (GPU) |
|----------|--------------|------------------|
| cpus-per-task | 128 | 32 |
| gpus | 0 | 1 |
| mem | 256G | 64G |
| time | 24:00:00 | 12:00:00 |
| partition | - | gpu |

---

## üöÄ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ GPU –≤–µ—Ä—Å–∏–∏

### –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è

| –≠—Ç–∞–ø | CPU | GPU (RTX 3090) | –£—Å–∫–æ—Ä–µ–Ω–∏–µ |
|------|-----|----------------|-----------|
| Small training | ~30 min | ~5-8 min | **4-6x** |
| Large training | ~60 min | ~15-20 min | **3-4x** |
| Small eval | ~5 min | ~2 min | **2.5x** |
| Large eval | ~5 min | ~2 min | **2.5x** |
| **Total pipeline** | **~20-24h** | **~8-10h** | **~2.5x** |

### –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏

- –ë–æ–ª—å—à–µ training steps ‚Üí –ª—É—á—à–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
- –ë–æ–ª—å—à–µ batch_size ‚Üí –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (max_num_ctx) ‚Üí –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π

### –†–µ—Å—É—Ä—Å—ã

- –ú–µ–Ω—å—à–µ CPU cores —Ç—Ä–µ–±—É–µ—Ç—Å—è (32 vs 128)
- –ú–µ–Ω—å—à–µ RAM —Ç—Ä–µ–±—É–µ—Ç—Å—è (64GB vs 256GB)
- –ë—ã—Å—Ç—Ä–µ–µ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç—Å—è –æ—á–µ—Ä–µ–¥—å SLURM

---

## üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏

### GPU Memory Usage (–ø—Ä–∏–º–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)

| –≠—Ç–∞–ø | Expected Usage |
|------|----------------|
| Small model training | ~2-3 GB |
| Large model training | ~6-8 GB |
| Evaluation | ~1-2 GB |
| Peak usage | ~8-10 GB |

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

1. **Automatic cache clearing**: GPU –∫—ç—à –æ—á–∏—â–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞
2. **Memory monitoring**: –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
3. **Error handling**: Graceful fallback –µ—Å–ª–∏ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
4. **Batch size optimization**: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ batch sizes –¥–ª—è —Ä–∞–∑–Ω—ã—Ö GPU

---

## üìù –ß—Ç–æ –ù–ï –∏–∑–º–µ–Ω–∏–ª–æ—Å—å

‚úì –ö–ª–∞—Å—Å `ClusterLDAExperimentRunner` - –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚úì –ö–ª–∞—Å—Å `ResultsAggregator` - –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚úì LDA —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã (Stage 5) - –≤—Å–µ –µ—â–µ –Ω–∞ CPU
‚úì –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (Stage 6) - –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚úì –§–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - —Å–æ–≤–º–µ—Å—Ç–∏–º —Å CPU –≤–µ—Ä—Å–∏–µ–π

**–ü–æ—á–µ–º—É LDA –Ω–∞ CPU?**
- LDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç GPU
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ CPU
- –û—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç GPU –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á

---

## üéØ –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫—É—é –≤–µ—Ä—Å–∏—é

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU –≤–µ—Ä—Å–∏—é (`for_klaster_gpu.py`) –µ—Å–ª–∏:
- ‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω GPU —Å 8+ GB –ø–∞–º—è—Ç–∏
- ‚úÖ –ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- ‚úÖ –ù—É–∂–Ω–∞ –ª—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π
- ‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CPU –≤–µ—Ä—Å–∏—é (`for_klaster.py`) –µ—Å–ª–∏:
- ‚úÖ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
- ‚úÖ –î–æ—Å—Ç—É–ø–Ω–æ –º–Ω–æ–≥–æ CPU cores (128+)
- ‚úÖ –ù–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
- ‚úÖ –•–æ—Ç–∏—Ç–µ —Å—ç–∫–æ–Ω–æ–º–∏—Ç—å GPU —Ä–µ—Å—É—Ä—Å—ã

---

## üîç –ö–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU

### –í –ª–æ–≥–∞—Ö –±—É–¥–µ—Ç:
```
============================================================================
STAGE 0: GPU Initialization
============================================================================
‚úì GPU Available: NVIDIA A100-SXM4-40GB
‚úì GPU Count: 1
‚úì CUDA Version: 11.8
‚úì PyTorch Version: 2.1.0
‚úì Total GPU Memory: 40.00 GB
============================================================================
```

### –í SLURM output:
```
GPU Information:
----------------------------------------------------------------------------
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:00:04.0 Off |                    0 |
| N/A   38C    P0    60W / 400W |   7856MiB / 40960MiB |     95%      Default |
+-------------------------------+----------------------+----------------------+
```

### –í pipeline_metrics.json:
```json
{
  "stages": {
    "PABBO_Training_Small_GPU": {
      "status": "SUCCESS",
      "device": "cuda",
      "gpu_memory_gb": 2.34
    }
  }
}
```

---

## üêõ –û–±—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### 1. "GPU not available"
**–ü—Ä–∏—á–∏–Ω–∞**: CUDA –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏–ª–∏ GPU –Ω–µ –≤–∏–¥–µ–Ω
**–†–µ—à–µ–Ω–∏–µ**: –°–º. GPU_SETUP.md —Ä–∞–∑–¥–µ–ª Troubleshooting

### 2. "CUDA out of memory"
**–ü—Ä–∏—á–∏–Ω–∞**: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏
**–†–µ—à–µ–Ω–∏–µ**: –£–º–µ–Ω—å—à–∏—Ç–µ batch_size –≤ –∫–æ–¥–µ

### 3. –ú–µ–¥–ª–µ–Ω–Ω–µ–µ —á–µ–º –æ–∂–∏–¥–∞–ª–æ—Å—å
**–ü—Ä–∏—á–∏–Ω–∞**: –í–æ–∑–º–æ–∂–Ω–æ CPU bottleneck
**–†–µ—à–µ–Ω–∏–µ**: –£–≤–µ–ª–∏—á—å—Ç–µ `cpus-per-task` –≤ run_gpu.sh

---

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ: `GPU_SETUP.md`
- –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –≥–∞–π–¥: `CLUSTER_README.md`
- –û–±—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å: `WORKFLOW_DIAGRAM.md`
- PABBO –æ–±—É—á–µ–Ω–∏–µ: `PABBO_TRAINING_GUIDE.md`

---

## ‚úÖ Checklist –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º GPU –≤–µ—Ä—Å–∏–∏

- [ ] –ü—Ä–æ–≤–µ—Ä–∏–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU (`nvidia-smi`)
- [ ] –ü—Ä–æ–≤–µ—Ä–∏–ª–∏ CUDA –≤–µ—Ä—Å–∏—é (11.0+)
- [ ] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω PyTorch —Å CUDA support
- [ ] –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏ (8+ GB)
- [ ] –û–±–Ω–æ–≤–ª–µ–Ω `run_gpu.sh` —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –∑–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ
- [ ] –°–æ–∑–¥–∞–Ω/–∑–∞–≥—Ä—É–∂–µ–Ω Docker –æ–±—Ä–∞–∑ —Å GPU support (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)

---

**–ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É! üéâ**

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:
```bash
python for_klaster_gpu.py           # –õ–æ–∫–∞–ª—å–Ω–æ
sbatch run_gpu.sh                   # –ù–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ
```