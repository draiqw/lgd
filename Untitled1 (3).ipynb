{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### MAIN_CODE"
   ],
   "metadata": {
    "id": "Cg21VFN0JynV"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tiVGOw-T9pEy",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:50:40.500305Z",
     "start_time": "2025-10-26T15:50:40.494038Z"
    }
   },
   "source": "import math\nimport time\nimport numpy as np\nimport scipy.sparse as sp\nfrom gensim.models import LdaModel\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport time, math\nimport numpy as np\nfrom typing import Callable, Tuple, Dict, List\nimport time\nimport numpy as np\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport time\nimport math\nimport random\nimport numpy as np\nfrom deap import base, creator, tools\nimport os\nimport csv\nimport json\nimport time\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom tensorboardX import SummaryWriter",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def load_bow_pair(train_path: str, val_path: str):\n",
    "    Xtr = sp.load_npz(train_path).tocsr(copy=False)\n",
    "    Xva = sp.load_npz(val_path).tocsr(copy=False)\n",
    "    return Xtr, Xva"
   ],
   "metadata": {
    "id": "VYDIrQLj-enq",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:50:43.187811Z",
     "start_time": "2025-10-26T15:50:43.185088Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "Xtr, Xva = load_bow_pair(\"data/X_agnews_train_bow.npz\", \"data/X_agnews_val_bow.npz\")",
   "metadata": {
    "id": "grnTmfsTBDFL",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:50:56.102584Z",
     "start_time": "2025-10-26T15:50:56.088401Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "def lda_blackbox(\n",
    "    T: int,\n",
    "    alpha: float,\n",
    "    eta: float,\n",
    "    *,\n",
    "    seed: int = 42,\n",
    "    max_iter: int = 400,\n",
    "    batch_size: int = 2048,\n",
    "    learning_method: str = \"online\"\n",
    "):\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=int(T),\n",
    "        doc_topic_prior=float(alpha),\n",
    "        topic_word_prior=float(eta),\n",
    "        learning_method=learning_method,\n",
    "        max_iter=int(max_iter),\n",
    "        batch_size=int(batch_size),\n",
    "        random_state=int(seed),\n",
    "        evaluate_every=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    lda.fit(Xtr)\n",
    "    fit_time = time.perf_counter() - t0\n",
    "    ppl = float(lda.perplexity(Xva))\n",
    "    return ppl"
   ],
   "metadata": {
    "id": "CKTS0AMIpP8O",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:51:35.785724Z",
     "start_time": "2025-10-26T15:51:35.780366Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)"
   ],
   "metadata": {
    "id": "KoQZPMccqB4V",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:51:39.259469Z",
     "start_time": "2025-10-26T15:51:39.256744Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "def _clamp(x, lo, hi):\n",
    "    return lo if x < lo else hi if x > hi else x"
   ],
   "metadata": {
    "id": "s7y7cFkCqFhZ",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:51:41.753507Z",
     "start_time": "2025-10-26T15:51:41.750388Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "class GAOptimizer:\n    def __init__(\n        self,\n        obj,\n        T_bounds=(10, 200),\n        alpha_bounds=(1e-3, 1.0),\n        eta_bounds=(1e-3, 1.0),\n        *,\n        log_space=True,\n        seed=42,\n        cxpb=0.9,\n        mutpb=0.2,\n        tournsize=3,\n        elite=2,\n        sigma_log=0.25,\n        dT=5\n    ):\n        self.obj = obj\n        self.Tb = T_bounds\n        self.ab = alpha_bounds\n        self.eb = eta_bounds\n        self.log = log_space\n        self.seed = int(seed)\n        self.cxpb = cxpb\n        self.mutpb = mutpb\n        self.tournsize = tournsize\n        self.elite = elite\n        self.sigma_log = sigma_log\n        self.dT = int(dT)\n        self.toolbox = base.Toolbox()\n        if self.log:\n            self._ab_log = (math.log10(self.ab[0]), math.log10(self.ab[1]))\n            self._eb_log = (math.log10(self.eb[0]), math.log10(self.eb[1]))\n        else:\n            self._ab_log = self.ab\n            self._eb_log = self.eb\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n        \n        # Custom individual initialization: alpha and eta are initialized as 1/T\n        def create_individual():\n            T = random.randint(self.Tb[0], self.Tb[1])\n            if self.log:\n                # In log-space: log10(1/T) = -log10(T)\n                init_alpha = -math.log10(T)\n                init_eta = -math.log10(T)\n                # Clamp to bounds\n                init_alpha = _clamp(init_alpha, self._ab_log[0], self._ab_log[1])\n                init_eta = _clamp(init_eta, self._eb_log[0], self._eb_log[1])\n            else:\n                init_alpha = _clamp(1.0 / T, self.ab[0], self.ab[1])\n                init_eta = _clamp(1.0 / T, self.eb[0], self.eb[1])\n            return creator.Individual([T, init_alpha, init_eta])\n        \n        self.toolbox.register(\"individual\", create_individual)\n        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n        self.toolbox.register(\"evaluate\", self._evaluate)\n        self.toolbox.register(\"select\", tools.selTournament, tournsize=self.tournsize)\n\n    def _decode(self, ind):\n        T = int(round(ind[0]))\n        T = _clamp(T, self.Tb[0], self.Tb[1])\n        if self.log:\n            a = 10.0 ** _clamp(ind[1], self._ab_log[0], self._ab_log[1])\n            e = 10.0 ** _clamp(ind[2], self._eb_log[0], self._eb_log[1])\n        else:\n            a = _clamp(ind[1], self.ab[0], self.ab[1])\n            e = _clamp(ind[2], self.eb[0], self.eb[1])\n        return T, float(a), float(e)\n\n    def _evaluate(self, ind):\n        T, a, e = self._decode(ind)\n        try:\n            v = float(self.obj(T, a, e))\n        except Exception:\n            v = float(\"inf\")\n        return (v,)\n\n    def _cx(self, ind1, ind2):\n        if random.random() < 0.5:\n            ind1[0], ind2[0] = ind2[0], ind1[0]\n        for j in (1, 2):\n            g = random.random()\n            a = ind1[j]\n            b = ind2[j]\n            ind1[j] = g * a + (1.0 - g) * b\n            ind2[j] = (1.0 - g) * a + g * b\n        return ind1, ind2\n\n    def _mut(self, ind):\n        if random.random() < 1.0:\n            ind[0] = _clamp(int(round(ind[0] + random.randint(-self.dT, self.dT))), self.Tb[0], self.Tb[1])\n        if random.random() < 1.0:\n            ind[1] = _clamp(ind[1] + random.gauss(0.0, self.sigma_log), self._ab_log[0], self._ab_log[1])\n        if random.random() < 1.0:\n            ind[2] = _clamp(ind[2] + random.gauss(0.0, self.sigma_log), self._eb_log[0], self._eb_log[1])\n        return (ind,)\n\n    def run(self, gens=200, pop_size=10):\n        print(f\"[GA] Starting optimization: {gens} generations, population size {pop_size}\")\n        print(f\"[GA] T bounds: {self.Tb}, alpha bounds: {self.ab}, eta bounds: {self.eb}\")\n        \n        pop = self.toolbox.population(n=pop_size)\n        hall = tools.HallOfFame(maxsize=self.elite)\n        history = []\n        t0 = time.perf_counter()\n        cum_time = 0.0\n        \n        # Early stopping variables\n        no_improvement_count = 0\n        prev_best_fitness = float('inf')\n        max_no_improvement = 5\n        \n        print(f\"[GA] Evaluating initial population...\")\n        for ind in pop:\n            ind.fitness.values = self.toolbox.evaluate(ind)\n        hall.update(pop)\n        best_sofar = min(pop, key=lambda x: x.fitness.values[0])\n        Tb, ab, eb = self._decode(best_sofar)\n        print(f\"[GA] Initial best: T={Tb}, alpha={ab:.6f}, eta={eb:.6f}, ppl={best_sofar.fitness.values[0]:.4f}\")\n        \n        for g in range(gens):\n            gs = time.perf_counter()\n            elites = tools.selBest(pop, self.elite)\n            offspring = self.toolbox.select(pop, len(pop) - self.elite)\n            offspring = list(map(self.toolbox.clone, offspring))\n            for i in range(0, len(offspring) - 1, 2):\n                if random.random() < self.cxpb:\n                    self._cx(offspring[i], offspring[i + 1])\n            for i in range(len(offspring)):\n                if random.random() < self.mutpb:\n                    self._mut(offspring[i])\n                del offspring[i].fitness.values\n            invalid = [ind for ind in offspring if not ind.fitness.valid]\n            for ind in invalid:\n                ind.fitness.values = self.toolbox.evaluate(ind)\n            pop = elites + offspring\n            hall.update(pop)\n            cur_best = min(pop, key=lambda x: x.fitness.values[0])\n            if cur_best.fitness.values[0] < best_sofar.fitness.values[0]:\n                best_sofar = cur_best\n            gen_time = time.perf_counter() - gs\n            cum_time = time.perf_counter() - t0\n            vals = [ind.fitness.values[0] for ind in pop]\n            Tb, ab, eb = self._decode(best_sofar)\n            \n            # Early stopping check\n            current_best_fitness = best_sofar.fitness.values[0]\n            if abs(current_best_fitness - prev_best_fitness) < 1e-9:\n                no_improvement_count += 1\n            else:\n                no_improvement_count = 0\n            prev_best_fitness = current_best_fitness\n            \n            # Console logging\n            print(f\"[GA] Gen {g+1}/{gens} | Best: T={Tb}, alpha={ab:.6f}, eta={eb:.6f}, ppl={best_sofar.fitness.values[0]:.4f} | \"\n                  f\"Pop: mean={np.mean(vals):.4f}, std={np.std(vals):.4f} | Time: {gen_time:.2f}s (total: {cum_time:.2f}s) | \"\n                  f\"No improvement: {no_improvement_count}/{max_no_improvement}\")\n            \n            history.append({\n                \"iter\": g,\n                \"best_ppl_sofar\": best_sofar.fitness.values[0],\n                \"pop_mean\": float(np.mean(vals)),\n                \"pop_std\": float(np.std(vals)),\n                \"T_best\": Tb,\n                \"alpha_best\": ab,\n                \"eta_best\": eb,\n                \"step_time\": gen_time,\n                \"cum_time\": cum_time\n            })\n            \n            # Check early stopping condition\n            if no_improvement_count >= max_no_improvement:\n                print(f\"[GA] Early stopping: best fitness unchanged for {max_no_improvement} iterations\")\n                break\n        \n        print(f\"[GA] Optimization complete! Total time: {cum_time:.2f}s\")\n        print(f\"[GA] Final best: T={Tb}, alpha={ab:.6f}, eta={eb:.6f}, ppl={best_sofar.fitness.values[0]:.4f}\")\n        \n        best = self._decode(best_sofar)\n        best_val = best_sofar.fitness.values[0]\n        final_T = best[0]\n        first_hit = None\n        for row in history:\n            if row[\"T_best\"] == final_T:\n                first_hit = row[\"cum_time\"]\n                break\n        return {\n            \"best\": {\"T\": best[0], \"alpha\": best[1], \"eta\": best[2], \"ppl\": best_val},\n            \"history\": history,\n            \"total_time\": history[-1][\"cum_time\"] if history else 0.0,\n            \"time_to_best_T\": first_hit if first_hit is not None else None\n        }",
   "metadata": {
    "id": "4Kdm9lmmqJf1",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:51:43.583373Z",
     "start_time": "2025-10-26T15:51:43.571427Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class ESOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        obj,\n",
    "        T_bounds=(10, 200),\n",
    "        alpha_bounds=(1e-3, 1.0),\n",
    "        eta_bounds=(1e-3, 1.0),\n",
    "        *,\n",
    "        log_space=True,\n",
    "        seed=42,\n",
    "        mu=12,\n",
    "        lmbda=48,\n",
    "        sigma_log=0.25,\n",
    "        dT=5\n",
    "    ):\n",
    "        self.obj = obj\n",
    "        self.Tb = T_bounds\n",
    "        self.ab = alpha_bounds\n",
    "        self.eb = eta_bounds\n",
    "        self.log = log_space\n",
    "        self.seed = int(seed)\n",
    "        self.mu = int(mu)\n",
    "        self.lmbda = int(lmbda)\n",
    "        self.sigma_log = sigma_log\n",
    "        self.dT = int(dT)\n",
    "        self.toolbox = base.Toolbox()\n",
    "        if self.log:\n",
    "            self._ab_log = (math.log10(self.ab[0]), math.log10(self.ab[1]))\n",
    "            self._eb_log = (math.log10(self.eb[0]), math.log10(self.eb[1]))\n",
    "        else:\n",
    "            self._ab_log = self.ab\n",
    "            self._eb_log = self.eb\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        # Custom individual initialization: alpha and eta are initialized as 1/T\n",
    "        def create_individual():\n",
    "            T = random.randint(self.Tb[0], self.Tb[1])\n",
    "            if self.log:\n",
    "                # In log-space: log10(1/T) = -log10(T)\n",
    "                init_alpha = -math.log10(T)\n",
    "                init_eta = -math.log10(T)\n",
    "                # Clamp to bounds\n",
    "                init_alpha = _clamp(init_alpha, self._ab_log[0], self._ab_log[1])\n",
    "                init_eta = _clamp(init_eta, self._eb_log[0], self._eb_log[1])\n",
    "            else:\n",
    "                init_alpha = _clamp(1.0 / T, self.ab[0], self.ab[1])\n",
    "                init_eta = _clamp(1.0 / T, self.eb[0], self.eb[1])\n",
    "            return creator.Individual([T, init_alpha, init_eta])\n",
    "        \n",
    "        self.toolbox.register(\"individual\", create_individual)\n",
    "        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n",
    "        self.toolbox.register(\"evaluate\", self._evaluate)\n",
    "\n",
    "    def _clamp(self, ind):\n",
    "        ind[0] = _clamp(int(round(ind[0])), self.Tb[0], self.Tb[1])\n",
    "        ind[1] = _clamp(ind[1], self._ab_log[0], self._ab_log[1])\n",
    "        ind[2] = _clamp(ind[2], self._eb_log[0], self._eb_log[1])\n",
    "\n",
    "    def _decode(self, ind):\n",
    "        T = int(round(ind[0]))\n",
    "        T = _clamp(T, self.Tb[0], self.Tb[1])\n",
    "        if self.log:\n",
    "            a = 10.0 ** _clamp(ind[1], self._ab_log[0], self._ab_log[1])\n",
    "            e = 10.0 ** _clamp(ind[2], self._eb_log[0], self._eb_log[1])\n",
    "        else:\n",
    "            a = _clamp(ind[1], self.ab[0], self.ab[1])\n",
    "            e = _clamp(ind[2], self.eb[0], self.eb[1])\n",
    "        return T, float(a), float(e)\n",
    "\n",
    "    def _evaluate(self, ind):\n",
    "        T, a, e = self._decode(ind)\n",
    "        try:\n",
    "            v = float(self.obj(T, a, e))\n",
    "        except Exception:\n",
    "            v = float(\"inf\")\n",
    "        return (v,)\n",
    "\n",
    "    def _mut(self, parent):\n",
    "        child = creator.Individual(parent[:])\n",
    "        child[0] = int(round(child[0] + random.randint(-self.dT, self.dT)))\n",
    "        child[1] = child[1] + random.gauss(0.0, self.sigma_log)\n",
    "        child[2] = child[2] + random.gauss(0.0, self.sigma_log)\n",
    "        self._clamp(child)\n",
    "        return child\n",
    "\n",
    "    def run(self, steps=24):\n",
    "        print(f\"[ES] Starting optimization: {steps} steps, mu={self.mu}, lambda={self.lmbda}\")\n",
    "        print(f\"[ES] T bounds: {self.Tb}, alpha bounds: {self.ab}, eta bounds: {self.eb}\")\n",
    "        \n",
    "        parents = self.toolbox.population(n=self.mu)\n",
    "        history = []\n",
    "        t0 = time.perf_counter()\n",
    "        cum_time = 0.0\n",
    "        \n",
    "        print(f\"[ES] Evaluating initial population...\")\n",
    "        for ind in parents:\n",
    "            ind.fitness.values = self.toolbox.evaluate(ind)\n",
    "        best_sofar = min(parents, key=lambda x: x.fitness.values[0])\n",
    "        Tb, ab, eb = self._decode(best_sofar)\n",
    "        print(f\"[ES] Initial best: T={Tb}, alpha={ab:.6f}, eta={eb:.6f}, ppl={best_sofar.fitness.values[0]:.4f}\")\n",
    "        \n",
    "        for s in range(steps):\n",
    "            gs = time.perf_counter()\n",
    "            off = []\n",
    "            for _ in range(self.lmbda):\n",
    "                p = random.choice(parents)\n",
    "                c = self._mut(p)\n",
    "                c.fitness.values = self.toolbox.evaluate(c)\n",
    "                off.append(c)\n",
    "            pool = parents + off\n",
    "            pool.sort(key=lambda x: x.fitness.values[0])\n",
    "            parents = [creator.Individual(ind[:]) for ind in pool[:self.mu]]\n",
    "            for i in range(self.mu):\n",
    "                parents[i].fitness.values = pool[i].fitness.values\n",
    "            cur_best = parents[0]\n",
    "            if cur_best.fitness.values[0] < best_sofar.fitness.values[0]:\n",
    "                best_sofar = cur_best\n",
    "            step_time = time.perf_counter() - gs\n",
    "            cum_time = time.perf_counter() - t0\n",
    "            vals = [ind.fitness.values[0] for ind in parents]\n",
    "            Tb, ab, eb = self._decode(best_sofar)\n",
    "            \n",
    "            # Console logging\n",
    "            print(f\"[ES] Step {s+1}/{steps} | Best: T={Tb}, alpha={ab:.6f}, eta={eb:.6f}, ppl={best_sofar.fitness.values[0]:.4f} | \"\n",
    "                  f\"Parents: mean={np.mean(vals):.4f}, std={np.std(vals):.4f} | Time: {step_time:.2f}s (total: {cum_time:.2f}s)\")\n",
    "            \n",
    "            history.append({\n",
    "                \"iter\": s,\n",
    "                \"best_ppl_sofar\": best_sofar.fitness.values[0],\n",
    "                \"pop_mean\": float(np.mean(vals)),\n",
    "                \"pop_std\": float(np.std(vals)),\n",
    "                \"T_best\": Tb,\n",
    "                \"alpha_best\": ab,\n",
    "                \"eta_best\": eb,\n",
    "                \"step_time\": step_time,\n",
    "                \"cum_time\": cum_time\n",
    "            })\n",
    "        \n",
    "        print(f\"[ES] Optimization complete! Total time: {cum_time:.2f}s\")\n",
    "        print(f\"[ES] Final best: T={Tb}, alpha={ab:.6f}, eta={eb:.6f}, ppl={best_sofar.fitness.values[0]:.4f}\")\n",
    "        \n",
    "        best = self._decode(best_sofar)\n",
    "        best_val = best_sofar.fitness.values[0]\n",
    "        final_T = best[0]\n",
    "        first_hit = None\n",
    "        for row in history:\n",
    "            if row[\"T_best\"] == final_T:\n",
    "                first_hit = row[\"cum_time\"]\n",
    "                break\n",
    "        return {\n",
    "            \"best\": {\"T\": best[0], \"alpha\": best[1], \"eta\": best[2], \"ppl\": best_val},\n",
    "            \"history\": history,\n",
    "            \"total_time\": history[-1][\"cum_time\"] if history else 0.0,\n",
    "            \"time_to_best_T\": first_hit if first_hit is not None else None\n",
    "        }"
   ],
   "metadata": {
    "id": "LCHio8DsqM9c",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:51:48.317430Z",
     "start_time": "2025-10-26T15:51:48.221324Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "EVAL_CACHE = {}\n\ndef _doc_perplexities(lda, X, batch_size=1024, eps=1e-300):\n    phi = lda.components_.astype(np.float64)\n    phi /= phi.sum(axis=1, keepdims=True)\n    n = X.shape[0]\n    out = np.empty(n, dtype=np.float64)\n    for s in range(0, n, batch_size):\n        e = min(n, s + batch_size)\n        Xb = X[s:e]\n        theta = lda.transform(Xb)\n        theta = np.clip(theta, 1e-12, None)\n        for i in range(Xb.shape[0]):\n            row = Xb[i]\n            idx = row.indices\n            dat = row.data\n            if dat.size == 0:\n                out[s + i] = 1.0\n                continue\n            p = theta[i].dot(phi[:, idx])\n            p = np.clip(p, eps, None)\n            ll = float((np.log(p) * dat).sum())\n            cnt = float(dat.sum())\n            out[s + i] = math.exp(-ll / max(cnt, 1.0))\n    return out\n\ndef _fit_eval_full(T, alpha, eta, seed=42, max_iter=400, batch_size=2048, learning_method=\"online\"):\n    key = (int(T), float(alpha), float(eta), int(seed), int(max_iter), int(batch_size), learning_method)\n    if key in EVAL_CACHE:\n        return EVAL_CACHE[key]\n    \n    print(f\"[LDA] Training LDA: T={T}, alpha={alpha:.6f}, eta={eta:.6f}\")\n    lda = LatentDirichletAllocation(\n        n_components=int(T),\n        doc_topic_prior=float(alpha),\n        topic_word_prior=float(eta),\n        learning_method=learning_method,\n        max_iter=int(max_iter),\n        batch_size=int(batch_size),\n        random_state=int(seed),\n        evaluate_every=-1,\n        n_jobs=-1\n    )\n    t0 = time.perf_counter()\n    lda.fit(Xtr)\n    fit_time = time.perf_counter() - t0\n    print(f\"[LDA] Training completed in {fit_time:.2f}s (n_iter={getattr(lda, 'n_iter_', 'N/A')})\")\n    \n    t1 = time.perf_counter()\n    # Evaluate on validation set\n    corpus_ppl_val = float(lda.perplexity(Xva))\n    doc_ppl_val = _doc_perplexities(lda, Xva, batch_size=min(1024, Xva.shape[0]))\n    \n    # Evaluate on train set\n    corpus_ppl_train = float(lda.perplexity(Xtr))\n    doc_ppl_train = _doc_perplexities(lda, Xtr, batch_size=min(1024, Xtr.shape[0]))\n    \n    eval_time = time.perf_counter() - t1\n    print(f\"[LDA] Evaluation completed in {eval_time:.2f}s | Train ppl: {corpus_ppl_train:.4f}, Val ppl: {corpus_ppl_val:.4f}\")\n    \n    res = {\n        \"T\": int(T),\n        \"alpha\": float(alpha),\n        \"eta\": float(eta),\n        \"corpus_ppl_val\": corpus_ppl_val,\n        \"corpus_ppl_train\": corpus_ppl_train,\n        \"doc_ppl_val_mean\": float(np.mean(doc_ppl_val)),\n        \"doc_ppl_val_max\": float(np.max(doc_ppl_val)),\n        \"doc_ppl_train_mean\": float(np.mean(doc_ppl_train)),\n        \"doc_ppl_train_max\": float(np.max(doc_ppl_train)),\n        \"fit_time\": fit_time,\n        \"eval_time\": eval_time,\n        \"n_iter_lda\": getattr(lda, \"n_iter_\", None)\n    }\n    EVAL_CACHE[key] = res\n    return res\n\ndef make_objective(seed=42, max_iter=400, batch_size=2048, learning_method=\"online\"):\n    def objective(T, a, e):\n        r = _fit_eval_full(T, a, e, seed=seed, max_iter=max_iter, batch_size=batch_size, learning_method=learning_method)\n        return r[\"corpus_ppl_val\"]\n    return objective\n\ndef _ensure_dir(p):\n    os.makedirs(p, exist_ok=True)\n\ndef _write_history_csv(history_rows, path):\n    fields = [\"iter\",\"best_corpus_ppl_val\",\"best_corpus_ppl_train\",\"best_doc_ppl_val_max\",\"pop_mean\",\"pop_std\",\"T_best\",\"alpha_best\",\"eta_best\",\"step_time\",\"cum_time\"]\n    with open(path, \"w\", newline=\"\") as f:\n        w = csv.DictWriter(f, fieldnames=fields)\n        w.writeheader()\n        for row in history_rows:\n            w.writerow(row)\n\ndef _plot_series(xs, ys, xlabel, ylabel, title, path):\n    plt.figure(figsize=(7,4))\n    plt.plot(xs, ys, marker=\"o\", linewidth=1.5)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(path, dpi=150)\n    plt.close()\n\ndef run_ga_with_logging(\n    outdir,\n    gens=200,\n    pop_size=10,\n    T_bounds=(10,200),\n    alpha_bounds=(1e-3,1.0),\n    eta_bounds=(1e-3,1.0),\n    seed=42,\n    max_iter=400,\n    batch_size=2048,\n    learning_method=\"online\",\n    cxpb=0.9,\n    mutpb=0.2,\n    tournsize=3,\n    elite=2,\n    sigma_log=0.25,\n    dT=5\n):\n    _ensure_dir(outdir)\n    writer = SummaryWriter(log_dir=os.path.join(outdir, \"tensorboard\"))\n    print(f\"[TensorBoard] Logging to {os.path.join(outdir, 'tensorboard')}\")\n    \n    obj = make_objective(seed=seed, max_iter=max_iter, batch_size=batch_size, learning_method=learning_method)\n    ga = GAOptimizer(\n        obj,\n        T_bounds=T_bounds,\n        alpha_bounds=alpha_bounds,\n        eta_bounds=eta_bounds,\n        log_space=True,\n        seed=seed,\n        cxpb=cxpb,\n        mutpb=mutpb,\n        tournsize=tournsize,\n        elite=elite,\n        sigma_log=sigma_log,\n        dT=dT\n    )\n    res = ga.run(gens=gens, pop_size=pop_size)\n    hist = []\n    for row in res[\"history\"]:\n        T = row[\"T_best\"]\n        a = row[\"alpha_best\"]\n        e = row[\"eta_best\"]\n        r = _fit_eval_full(T, a, e, seed=seed, max_iter=max_iter, batch_size=batch_size, learning_method=learning_method)\n        \n        # TensorBoard logging\n        iter_num = row[\"iter\"]\n        writer.add_scalar(\"Perplexity/val_corpus\", r[\"corpus_ppl_val\"], iter_num)\n        writer.add_scalar(\"Perplexity/train_corpus\", r[\"corpus_ppl_train\"], iter_num)\n        writer.add_scalar(\"Perplexity/val_doc_max\", r[\"doc_ppl_val_max\"], iter_num)\n        writer.add_scalar(\"Perplexity/train_doc_mean\", r[\"doc_ppl_train_mean\"], iter_num)\n        writer.add_scalar(\"Parameters/T\", T, iter_num)\n        writer.add_scalar(\"Parameters/alpha\", a, iter_num)\n        writer.add_scalar(\"Parameters/eta\", e, iter_num)\n        writer.add_scalar(\"Population/mean\", row[\"pop_mean\"], iter_num)\n        writer.add_scalar(\"Population/std\", row[\"pop_std\"], iter_num)\n        writer.add_scalar(\"Time/step_time\", row[\"step_time\"], iter_num)\n        writer.add_scalar(\"Time/cumulative\", row[\"cum_time\"], iter_num)\n        \n        hist.append({\n            \"iter\": row[\"iter\"],\n            \"best_corpus_ppl_val\": float(r[\"corpus_ppl_val\"]),\n            \"best_corpus_ppl_train\": float(r[\"corpus_ppl_train\"]),\n            \"best_doc_ppl_val_max\": float(r[\"doc_ppl_val_max\"]),\n            \"pop_mean\": row[\"pop_mean\"],\n            \"pop_std\": row[\"pop_std\"],\n            \"T_best\": int(T),\n            \"alpha_best\": float(a),\n            \"eta_best\": float(e),\n            \"step_time\": row[\"step_time\"],\n            \"cum_time\": row[\"cum_time\"]\n        })\n    \n    writer.close()\n    print(f\"[TensorBoard] TensorBoard logs saved. Run: tensorboard --logdir={os.path.join(outdir, 'tensorboard')}\")\n    \n    _write_history_csv(hist, os.path.join(outdir, \"history.csv\"))\n    xs = [h[\"iter\"] for h in hist]\n    ys_val = [h[\"best_corpus_ppl_val\"] for h in hist]\n    ys_train = [h[\"best_corpus_ppl_train\"] for h in hist]\n    ys_max = [h[\"best_doc_ppl_val_max\"] for h in hist]\n    _plot_series(xs, ys_val, \"iter\", \"perplexity\", \"GA: val corpus perplexity vs iter\", os.path.join(outdir, \"val_ppl.png\"))\n    _plot_series(xs, ys_train, \"iter\", \"perplexity\", \"GA: train corpus perplexity vs iter\", os.path.join(outdir, \"train_ppl.png\"))\n    _plot_series(xs, ys_max, \"iter\", \"perplexity\", \"GA: max doc perplexity (val) vs iter\", os.path.join(outdir, \"max_ppl.png\"))\n    avg_step_time = float(np.mean([h[\"step_time\"] for h in hist])) if hist else 0.0\n    summary = {\n        \"best\": res[\"best\"],\n        \"avg_step_time\": avg_step_time,\n        \"total_time\": res[\"total_time\"],\n        \"time_to_best_T\": res[\"time_to_best_T\"]\n    }\n    with open(os.path.join(outdir, \"summary.json\"), \"w\") as f:\n        json.dump(summary, f, indent=2)\n    return {\"history\": hist, \"summary\": summary}\n\ndef run_es_with_logging(\n    outdir,\n    steps=24,\n    T_bounds=(10,200),\n    alpha_bounds=(1e-3,1.0),\n    eta_bounds=(1e-3,1.0),\n    seed=42,\n    max_iter=400,\n    batch_size=2048,\n    learning_method=\"online\",\n    mu=12,\n    lmbda=48,\n    sigma_log=0.25,\n    dT=5\n):\n    _ensure_dir(outdir)\n    writer = SummaryWriter(log_dir=os.path.join(outdir, \"tensorboard\"))\n    print(f\"[TensorBoard] Logging to {os.path.join(outdir, 'tensorboard')}\")\n    \n    obj = make_objective(seed=seed, max_iter=max_iter, batch_size=batch_size, learning_method=learning_method)\n    es = ESOptimizer(\n        obj,\n        T_bounds=T_bounds,\n        alpha_bounds=alpha_bounds,\n        eta_bounds=eta_bounds,\n        log_space=True,\n        seed=seed,\n        mu=mu,\n        lmbda=lmbda,\n        sigma_log=sigma_log,\n        dT=dT\n    )\n    res = es.run(steps=steps)\n    hist = []\n    for row in res[\"history\"]:\n        T = row[\"T_best\"]\n        a = row[\"alpha_best\"]\n        e = row[\"eta_best\"]\n        r = _fit_eval_full(T, a, e, seed=seed, max_iter=max_iter, batch_size=batch_size, learning_method=learning_method)\n        \n        # TensorBoard logging\n        iter_num = row[\"iter\"]\n        writer.add_scalar(\"Perplexity/val_corpus\", r[\"corpus_ppl_val\"], iter_num)\n        writer.add_scalar(\"Perplexity/train_corpus\", r[\"corpus_ppl_train\"], iter_num)\n        writer.add_scalar(\"Perplexity/val_doc_max\", r[\"doc_ppl_val_max\"], iter_num)\n        writer.add_scalar(\"Perplexity/train_doc_mean\", r[\"doc_ppl_train_mean\"], iter_num)\n        writer.add_scalar(\"Parameters/T\", T, iter_num)\n        writer.add_scalar(\"Parameters/alpha\", a, iter_num)\n        writer.add_scalar(\"Parameters/eta\", e, iter_num)\n        writer.add_scalar(\"Population/mean\", row[\"pop_mean\"], iter_num)\n        writer.add_scalar(\"Population/std\", row[\"pop_std\"], iter_num)\n        writer.add_scalar(\"Time/step_time\", row[\"step_time\"], iter_num)\n        writer.add_scalar(\"Time/cumulative\", row[\"cum_time\"], iter_num)\n        \n        hist.append({\n            \"iter\": row[\"iter\"],\n            \"best_corpus_ppl_val\": float(r[\"corpus_ppl_val\"]),\n            \"best_corpus_ppl_train\": float(r[\"corpus_ppl_train\"]),\n            \"best_doc_ppl_val_max\": float(r[\"doc_ppl_val_max\"]),\n            \"pop_mean\": row[\"pop_mean\"],\n            \"pop_std\": row[\"pop_std\"],\n            \"T_best\": int(T),\n            \"alpha_best\": float(a),\n            \"eta_best\": float(e),\n            \"step_time\": row[\"step_time\"],\n            \"cum_time\": row[\"cum_time\"]\n        })\n    \n    writer.close()\n    print(f\"[TensorBoard] TensorBoard logs saved. Run: tensorboard --logdir={os.path.join(outdir, 'tensorboard')}\")\n    \n    _write_history_csv(hist, os.path.join(outdir, \"history.csv\"))\n    xs = [h[\"iter\"] for h in hist]\n    ys_val = [h[\"best_corpus_ppl_val\"] for h in hist]\n    ys_train = [h[\"best_corpus_ppl_train\"] for h in hist]\n    ys_max = [h[\"best_doc_ppl_val_max\"] for h in hist]\n    _plot_series(xs, ys_val, \"iter\", \"perplexity\", \"ES: val corpus perplexity vs iter\", os.path.join(outdir, \"val_ppl.png\"))\n    _plot_series(xs, ys_train, \"iter\", \"perplexity\", \"ES: train corpus perplexity vs iter\", os.path.join(outdir, \"train_ppl.png\"))\n    _plot_series(xs, ys_max, \"iter\", \"perplexity\", \"ES: max doc perplexity (val) vs iter\", os.path.join(outdir, \"max_ppl.png\"))\n    avg_step_time = float(np.mean([h[\"step_time\"] for h in hist])) if hist else 0.0\n    summary = {\n        \"best\": res[\"best\"],\n        \"avg_step_time\": avg_step_time,\n        \"total_time\": res[\"total_time\"],\n        \"time_to_best_T\": res[\"time_to_best_T\"]\n    }\n    with open(os.path.join(outdir, \"summary.json\"), \"w\") as f:\n        json.dump(summary, f, indent=2)\n    return {\"history\": hist, \"summary\": summary}",
   "metadata": {
    "id": "eE0uE7RHrloY",
    "ExecuteTime": {
     "end_time": "2025-10-26T15:52:20.350054Z",
     "start_time": "2025-10-26T15:52:20.336357Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "BASE_DIR = \"runs/agnews\"\n\nga_out = run_ga_with_logging(\n    outdir=f\"{BASE_DIR}/ga\",\n    seed=42,\n    max_iter=400,\n    batch_size=2048,\n    learning_method=\"online\"\n)\n\nes_out = run_es_with_logging(\n    outdir=f\"{BASE_DIR}/es\",\n    steps=24,\n    seed=42,\n    max_iter=400,\n    batch_size=2048,\n    learning_method=\"online\"\n)\n\nprint(\"GA summary:\", ga_out[\"summary\"])\nprint(\"ES summary:\", es_out[\"summary\"])",
   "metadata": {
    "id": "9IY9-TT8stTs"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}