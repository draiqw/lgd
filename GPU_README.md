# üöÄ GPU Version - Quick Start

–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –¥–ª—è GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ PABBO pipeline.

---

## üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

| –§–∞–π–ª | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
|------|------------|
| **`for_klaster_gpu.py`** | –û—Å–Ω–æ–≤–Ω–æ–π GPU-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç |
| **`run_gpu.sh`** | SLURM —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ GPU –Ω–æ–¥–µ |
| **`test_gpu.py`** | –¢–µ—Å—Ç GPU –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã |
| **`GPU_SETUP.md`** | –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é |
| **`GPU_CHANGES_SUMMARY.md`** | –î–µ—Ç–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ –≤—Å–µ—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π |
| **`GPU_README.md`** | –≠—Ç–æ—Ç —Ñ–∞–π–ª - –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç |

---

## ‚ö° –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç GPU
python test_gpu.py
```

–ï—Å–ª–∏ –≤—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏ ‚úì - –º–æ–∂–µ—Ç–µ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å!

### 2Ô∏è‚É£ –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ (—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ pipeline
python for_klaster_gpu.py
```

### 3Ô∏è‚É£ –ó–∞–ø—É—Å–∫ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ

```bash
# –û—Ç–ø—Ä–∞–≤—å—Ç–µ –∑–∞–¥–∞—á—É –≤ SLURM
sbatch run_gpu.sh

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
squeue -u $USER
tail -f slurm-*.out
```

---

## üéØ –ß—Ç–æ –¥–µ–ª–∞–µ—Ç GPU –≤–µ—Ä—Å–∏—è

### –û–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –Ω–∞ GPU

**Small Model** (–¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤):
- Transformer: d_model=32, nhead=2, n_layers=3
- Training: 4000 steps, batch_size=32
- –í—Ä–µ–º—è: ~5-8 –º–∏–Ω—É—Ç –Ω–∞ RTX 3090

**Large Model** (–¥–ª—è production):
- Transformer: d_model=64, nhead=4, n_layers=6
- Training: 12000 steps, batch_size=64
- –í—Ä–µ–º—è: ~15-20 –º–∏–Ω—É—Ç –Ω–∞ RTX 3090

### –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω

```
Stage 0: GPU Initialization         (1 –º–∏–Ω)
    ‚Üì
Stage 1: Train Small Model on GPU   (5-8 –º–∏–Ω)
    ‚Üì
Stage 2: Eval Small Model on GPU    (2 –º–∏–Ω)
    ‚Üì
Stage 3: Train Large Model on GPU   (15-20 –º–∏–Ω)
    ‚Üì
Stage 4: Eval Large Model on GPU    (2 –º–∏–Ω)
    ‚Üì
Stage 5: LDA Experiments (4 cores)  (8-10 —á–∞—Å–æ–≤)
    ‚Üì
Stage 6: Aggregate Results          (5 –º–∏–Ω)

TOTAL: ~8-10 —á–∞—Å–æ–≤ (vs 20-24 —á–∞—Å–∞ –Ω–∞ CPU)
```

---

## üí° –û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

### –°–∫–æ—Ä–æ—Å—Ç—å
- **2.5-3x** –±—ã—Å—Ç—Ä–µ–µ –æ–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
- **4-6x** –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ small model
- **3-4x** –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ large model

### –ö–∞—á–µ—Å—Ç–≤–æ
- **+100%** –±–æ–ª—å—à–µ training steps
- **+100%** –±–æ–ª—å—à–µ batch size
- **+150%** –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- ‚Üí –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π

### –†–µ—Å—É—Ä—Å—ã
- –ú–µ–Ω—å—à–µ CPU cores (32 vs 128)
- –ú–µ–Ω—å—à–µ RAM (64GB vs 256GB)
- –ë—ã—Å—Ç—Ä–µ–µ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç—Å—è –≤ SLURM

---

## üîß –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ú–∏–Ω–∏–º—É–º
- GPU: 8+ GB (GTX 1080 Ti, RTX 2080)
- CUDA: 11.0+
- PyTorch: 2.0+ —Å CUDA

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
- GPU: 16+ GB (RTX 3090, A100, V100)
- CUDA: 11.8 –∏–ª–∏ 12.1
- PyTorch: 2.1+ —Å CUDA

### –ü—Ä–æ–≤–µ—Ä–∫–∞
```bash
nvidia-smi                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
nvcc --version                # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
python test_gpu.py            # –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
```

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: CPU vs GPU

| –ú–µ—Ç—Ä–∏–∫–∞ | CPU | GPU | –£–ª—É—á—à–µ–Ω–∏–µ |
|---------|-----|-----|-----------|
| Small training | 30 –º–∏–Ω | 5-8 –º–∏–Ω | 4-6x ‚ö° |
| Large training | 60 –º–∏–Ω | 15-20 –º–∏–Ω | 3-4x ‚ö° |
| Total time | 20-24h | 8-10h | 2.5x ‚ö° |
| Training steps (small) | 2000 | 4000 | +100% üìà |
| Training steps (large) | 8000 | 12000 | +50% üìà |
| Batch size (small) | 16 | 32 | +100% üìà |
| Batch size (large) | 32 | 64 | +100% üìà |
| Context size | 20 | 50 | +150% üìà |
| CPU cores | 128 | 32 | -75% üí∞ |
| RAM | 256GB | 64GB | -75% üí∞ |

---

## üìù –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ

```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
python test_gpu.py

# 2. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (—Ç–æ–ª—å–∫–æ –æ–±—É—á–µ–Ω–∏–µ, –±–µ–∑ LDA)
# –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ for_klaster_gpu.py:
# - –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ STAGE 5 –∏ STAGE 6
python for_klaster_gpu.py

# 3. –ü–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫
python for_klaster_gpu.py
```

### –ó–∞–ø—É—Å–∫ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ

```bash
# 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ (–æ–¥–∏–Ω —Ä–∞–∑)
# - –°–æ–∑–¥–∞–π—Ç–µ Docker –æ–±—Ä–∞–∑ —Å GPU support
# - –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ –≤ .sqsh —á–µ—Ä–µ–∑ enroot
# - –ó–∞–≥—Ä—É–∑–∏—Ç–µ –Ω–∞ slurm-master

# 2. –ó–∞–ø—É—Å–∫
sbatch run_gpu.sh

# 3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
squeue -u $USER              # –°—Ç–∞—Ç—É—Å
tail -f slurm-*.out          # –õ–æ–≥–∏
ssh node "nvidia-smi"        # GPU usage
```

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

```python
# –í for_klaster_gpu.py

# –£–º–µ–Ω—å—à–∏—Ç—å batch_size (–µ—Å–ª–∏ –º–∞–ª–æ –ø–∞–º—è—Ç–∏)
batch_size = 16  # –≤–º–µ—Å—Ç–æ 32 –¥–ª—è small
batch_size = 32  # –≤–º–µ—Å—Ç–æ 64 –¥–ª—è large

# –£–≤–µ–ª–∏—á–∏—Ç—å training steps (–¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)
n_steps = 6000   # –≤–º–µ—Å—Ç–æ 4000 –¥–ª—è small
n_steps = 16000  # –≤–º–µ—Å—Ç–æ 12000 –¥–ª—è large
```

---

## üêõ Troubleshooting

### GPU –Ω–µ –Ω–∞–π–¥–µ–Ω
```
‚ùå GPU not available!
```
**–†–µ—à–µ–Ω–∏–µ**: –°–º. GPU_SETUP.md ‚Üí Troubleshooting ‚Üí "GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω"

### –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏
```
RuntimeError: CUDA out of memory
```
**–†–µ—à–µ–Ω–∏–µ**: –£–º–µ–Ω—å—à–∏—Ç–µ batch_size –≤ for_klaster_gpu.py

### –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞
**–ü—Ä–∏—á–∏–Ω—ã**: –ú–∞–ª–µ–Ω—å–∫–∏–π batch_size, CPU bottleneck
**–†–µ—à–µ–Ω–∏–µ**: –£–≤–µ–ª–∏—á—å—Ç–µ batch_size, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ CPU cores

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –û—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **GPU_SETUP.md** - –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ (–ù–ê–ß–ù–ò–¢–ï –° –≠–¢–û–ì–û!)
- **GPU_CHANGES_SUMMARY.md** - –î–µ—Ç–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
- **CLUSTER_README.md** - –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–µ

### PABBO –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **PABBO_TRAINING_GUIDE.md** - –û–±—É—á–µ–Ω–∏–µ PABBO
- **PABBO_PRETRAINING_GUIDE.md** - –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ
- **QUICK_START_PABBO.md** - –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç PABBO

### –î—Ä—É–≥–æ–µ
- **WORKFLOW_DIAGRAM.md** - –î–∏–∞–≥—Ä–∞–º–º–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞
- **EXPERIMENT_SPECIFICATION.md** - –°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

---

## üéì –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

### –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU

```bash
# –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
nvidia-smi

# –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
nvidia-smi -q

# –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
watch -n 1 nvidia-smi

# –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
nvcc --version

# –ü—Ä–æ–≤–µ—Ä–∫–∞ PyTorch
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
```

### SLURM –∫–æ–º–∞–Ω–¥—ã

```bash
# –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É
sbatch run_gpu.sh

# –°—Ç–∞—Ç—É—Å
squeue -u $USER

# –î–µ—Ç–∞–ª–∏ –∑–∞–¥–∞—á–∏
scontrol show job <jobid>

# –û—Ç–º–µ–Ω–∏—Ç—å –∑–∞–¥–∞—á—É
scancel <jobid>

# –õ–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
tail -f slurm-<jobid>.out
```

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```bash
# –ü–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—É—Å–∫
ls -lht lda_pipeline_results/ | head

# –õ–æ–≥–∏
tail -f lda_pipeline_results/run_gpu_*/logs/pipeline_main.log

# –ú–µ—Ç—Ä–∏–∫–∏
cat lda_pipeline_results/run_gpu_*/logs/pipeline_metrics.json | jq
```

---

## ‚úÖ Checklist –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º

### –õ–æ–∫–∞–ª—å–Ω–æ
- [ ] ‚úì GPU –¥–æ—Å—Ç—É–ø–µ–Ω (`nvidia-smi`)
- [ ] ‚úì CUDA —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (`nvcc --version`)
- [ ] ‚úì PyTorch —Å CUDA (`python test_gpu.py`)
- [ ] ‚úì –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (`pip list`)
- [ ] ‚úì –î–∞–Ω–Ω—ã–µ –Ω–∞ –º–µ—Å—Ç–µ (`ls data/`)

### –ù–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ
- [ ] ‚úì Docker –æ–±—Ä–∞–∑ —Å–æ–∑–¥–∞–Ω —Å GPU support
- [ ] ‚úì –û–±—Ä–∞–∑ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ .sqsh (enroot)
- [ ] ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ slurm-master
- [ ] ‚úì –û–±–Ω–æ–≤–ª–µ–Ω run_gpu.sh (–ø—É—Ç–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
- [ ] ‚úì –ü—Ä–æ–≤–µ—Ä–µ–Ω –∑–∞–ø—Ä–æ—Å GPU (#SBATCH --gpus=1)

---

## üÜö –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫—É—é –≤–µ—Ä—Å–∏—é?

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU –≤–µ—Ä—Å–∏—é –µ—Å–ª–∏:
‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω (8+ GB)
‚úÖ –ù—É–∂–Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å
‚úÖ –ù—É–∂–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å
‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –≤—Ä–µ–º—è

### –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CPU –≤–µ—Ä—Å–∏—é –µ—Å–ª–∏:
‚úÖ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
‚úÖ –ú–Ω–æ–≥–æ CPU cores (128+)
‚úÖ –ù–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
‚úÖ –≠–∫–æ–Ω–æ–º–∏—è GPU —Ä–µ—Å—É—Ä—Å–æ–≤

---

## üìû –ù—É–∂–Ω–∞ –ø–æ–º–æ—â—å?

1. **–ü—Ä–æ–±–ª–µ–º—ã —Å GPU**: –ß–∏—Ç–∞–π—Ç–µ GPU_SETUP.md ‚Üí Troubleshooting
2. **–ü—Ä–æ–±–ª–µ–º—ã —Å –∫–ª–∞—Å—Ç–µ—Ä–æ–º**: –ß–∏—Ç–∞–π—Ç–µ CLUSTER_README.md
3. **–ü—Ä–æ–±–ª–µ–º—ã —Å PABBO**: –ß–∏—Ç–∞–π—Ç–µ PABBO_TRAINING_GUIDE.md
4. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç**: `python test_gpu.py`

---

## üéØ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏

```bash
# ONE-LINER –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ–≥–æ
python test_gpu.py && echo "‚úì Ready!" || echo "‚úó Not ready"
```

–ï—Å–ª–∏ –≤—ã–≤–æ–¥ `‚úì Ready!` - –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å–∫–∞—Ç—å:
```bash
python for_klaster_gpu.py      # –õ–æ–∫–∞–ª—å–Ω–æ
sbatch run_gpu.sh              # –ù–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ
```

---

**–£—Å–ø–µ—à–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤! üöÄ**